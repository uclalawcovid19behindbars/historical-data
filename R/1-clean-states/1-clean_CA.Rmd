---
title: "Clean California - COVID-19 Behind Bars Historical Data Cleaning"
author: "Hope Johnson"
date: "2/3/21"
output: html_document
---

```{r package setup, include=FALSE}
##Define package list
Packages<-c("tidyverse", "glue", "assertthat", "stringr", "lubridate",
            "devtools", "magrittr", "skimr")
.packages = Packages
##Install CRAN packages (if not already installed)
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) install.packages(.packages[!.inst])
##Load packages into session 
lapply(.packages, require, character.only=TRUE)
devtools::install_github("uclalawcovid19behindbars/behindbarstools")
help(package=behindbarstools)
```

## Intro & Credits

This script is used to clean one state in the historical data concerning COVID-19 in state and federal prisons. Contributors to the historical data cleaning efforts include Hope Johnson, Michael Everett, Neal Marquez, Chase Hommeyer, Grace DiLaura, and Kalind Parish. Contributors to larger project include Sharon Dolovich, Aaron Littman, Danielle Flores, Poornima Rajeshwar, Victoria Rossi, and many others. 


## Load inputs

Input files: 

* Utilities script
* Historical data

```{r load inputs, echo=FALSE}
base_path <- file.path("~", "UCLA", "code", "historical", "historical-data")
data_path <- file.path(base_path, "data", "inputs")
##Load utilities function
util_path <- file.path(base_path, "R", "0-utilities.R")
source(util_path, local = knitr::knit_global())
```


```{r load data, message=FALSE}
df <- load_data(data_path, 
                "11420",
                filter_state = "California") 
df_typed <- type_convert(df)
```

Fill in missing values of `Staff.Deaths` with non-missing values of `Staff.Death`, when those exist. Do the same with `Resident.Death`/`Resident.Deaths`. 

```{r initial cleaning}
df_out <- df_typed %>%
  select(!starts_with("...")) %>%
  select(!starts_with("lots")) %>%
  select(!starts_with("Bad")) %>%
  select(!c("V2", "V4", "V5", "V7", "V8", "V10")) %>%
  select(!c("Facility.", "Coder", "Housing.Type")) %>%
  select_if(~sum(!is.na(.)) > 0) # rm 100% missing cols 
names(df_out) 

df_out %<>% 
  mutate(Resident.Deaths = behindbarstools::coalesce_with_warnings(Resident.Deaths, Resident.Death),
         Staff.Quarantine = behindbarstools::coalesce_with_warnings(Staff.Quarantine, Staff.Quarantined)) 
```

```{r create date var}
df_out <- df_out %>%
  mutate(date = as_date(sheet_name, format = "%Om.%d.%y"))
```

```{r standardize facility names}
df_cln <- df_out %>%
  mutate(
    Name = str_replace(Name, "-CDCR", "CDCR"),
    Name = str_replace(Name, "CDCRCCHCS", "CDCR CCHCS"),
    Name = str_replace(Name, "CDCR/CCHCS", "CDCR CCHS"))

df_mid <- behindbarstools::clean_facility_name(df_cln, debug = TRUE) 
```

```{r filter out federal facilities}
df_nonfederal <- df_mid %>%
  filter(federal_bool == FALSE)
```

```{r check name mismatches}
# show instances where merge didn't identify a clean name
df_nonfederal %>%
  filter(name_match == FALSE) %>% 
  select(scrape_name_clean) %>%
  unique()
```

Remove observations where we didn't pick up a clean name.

```{r drop null name observations}
df_filt <- df_nonfederal %>%
  filter(!is.na(scrape_name_clean)) %>%
  filter((name_match == TRUE) | (str_detect(Name, "CDCR CCHS"))) %>%
  mutate(Jurisdiction = ifelse(name_match == FALSE, "state", Jurisdiction)) # CDCR CCHS ones are "state"
```

Figure out duplicate date/facilities, concatenate those instances from multiple rows into one. This most often occurs because we scraped death data and infections data from separate tables.

```{r concat duplicate date/facilities}
nrow(distinct(df_filt, date, Name))
see_if(nrow(df_filt) == nrow(distinct(df_filt, Name, date)))

df_comb <- df_filt %>% 
  behindbarstools::group_by_coalesce(., Name, date, 
                                     .ignore = c("scrape_name_clean", "Facility", "source"))

assert_that(nrow(df_comb) == nrow(distinct(df_comb, Name, date)))
```

Filter down and re-order columns in order to row bind them to latest data.

```{r}
df_hist <- behindbarstools::reorder_cols(df_comb)
df_hist_final <- df_hist %>%
  mutate(source = Website,
         Residents.Deaths = Resident.Deaths,
         Date = date) %>%
  select(-c(Website, Resident.Deaths, date, Count.ID, Facility)) 
## add back in Residents.Tested (this will only be present for historical data)
df_hist_final$Residents.Tested <- df_hist$Residents.Tested
df_hist_final$Residents.Population <- df_hist$Residents.Population
df_hist_final <- behindbarstools::reorder_cols(df_hist_final, rm_extra_cols = TRUE) %>%
    select_if(~sum(!is.na(.)) > 0) # rm 100% missing cols. doing this because we don't want all logical cols for the row_bind
```

Add in more recently scraped data (from November until present). 

First, read it in and clean it. 

```{r add in recently scraped data}
recent_dat <- behindbarstools::read_scrape_data(all_dates = TRUE, 
                                                state = "California", 
                                                debug = TRUE) %>%
  filter(Jurisdiction != "federal",
       Date > as.Date('2020-11-04')) 

## view facility names without a match in the xwalk
recent_dat %>%
  filter(name_match == "FALSE") %>% 
  select(scrape_name_clean, Name) %>%
  unique()

recent_final <- behindbarstools::reorder_cols(recent_dat, rm_extra_cols = TRUE)
```
Then, bind it to the historical cleaned data from this script. 

```{r bind historical and recent}
all_equal(df_hist_final, recent_final, ignore_col_order = FALSE)
all_dat <- bind_rows(df_hist_final, recent_final)
n_distinct(all_dat$Name)
```
Merge in facility information (address, city, county, etc.).

```{r}
fac_info <- behindbarstools::read_fac_info() 
final_dat <- left_join(all_dat, fac_info, 
                  by = "Facility.ID",
                  suffix = c(".x", ""))

final_dat <- final_dat %>% 
  mutate(State = behindbarstools::coalesce_with_warnings(State.x, State),
         Name = behindbarstools::coalesce_with_warnings(Name.x, Name),
         Zipcode = behindbarstools::coalesce_with_warnings(Zipcode.x, Zipcode),
         Jurisdiction = behindbarstools::coalesce_with_warnings(Jurisdiction.x, Jurisdiction)) %>%
  select(-ends_with(".x"))
```

Look at the change in cases and deaths from day-to-day.

```{r plot cases/deaths}
test <- flag_noncumulative_cases(final_dat, Name)
test <- flag_noncumulative_deaths(test, Name, Residents.Deaths)

# lag cases overall 
test %>%
  filter(Name != "STATEWIDE") %>%
  ggplot(data = ., 
         aes(x = Date, y = lag_change_cases, group = Name)) +
    geom_line(alpha=0.6 , size=.5, color = "black") +
         scale_x_date(date_minor_breaks = "1 month", date_labels = "%m/%y", 
                      date_breaks = "1 month") + 
    labs(x = "Date",
      y = "lag_change_cases")

# lag deaths overall
test %>%
  filter(Name != "STATEWIDE") %>%
  ggplot(data = ., 
         aes(x = Date, y = lag_change_deaths, group = Name)) +
    geom_line(alpha=0.6 , size=.5, color = "black") +
    scale_x_date(date_labels = "%m") + 
    labs(x = "Date",
      y = "lag_change_deaths")

# plot lag counts by facility
lag_case_plots <- plot_lags(test, "Date", 
                            y_var = "lag_change_cases",
                            grp_var = Name)
## save lag case plots
for (i in 1:nrow(lag_case_plots)){
  facility_name <- lag_case_plots$Name[[i]]
  ggsave(paste0(facility_name, "_LagChangeCases.png"), lag_case_plots$plot[[i]],
         path = file.path(base_path, "plots", "CA", "cases"))
}
```


```{r plot cleaned death data}
testing <- flag_noncumulative_deaths(final_dat, Name, Residents.Deaths)
lag_death_plots <- plot_lags(testing, "Date", 
                             y_var = "lag_change_deaths",
                             grp_var = Name)

## save death plots
for (i in 1:nrow(lag_death_plots)){
  facility_name <- lag_death_plots$Name[[i]]
  ggsave(paste0(facility_name, "_LagChangeDeaths.png"), lag_death_plots$plot[[i]],
         path = file.path(base_path, "plots", "CA", "deaths"))
}

testing %>%
  filter(Name != "STATEWIDE") %>%
  ggplot(data = .,
         aes(x = Date, y = lag_change_deaths, group = Name)) +
    geom_line(alpha=0.6 , size=.5, color = "black") +
    scale_x_date(date_labels = "%m") +
    labs(x = "Date",
      y = "lag_change_deaths")
```

Find date spans / week spans with no data. In instances where the count went down by one, it could be that a PDF was misread. 

```{r}
dates <- final_dat %>%
  arrange(Date) %>%
  count(Date)
dates

ggplot(data = dates, 
       aes(x = Date, y = n)) +
  geom_bar(stat="identity") +
  labs(x = "Date",
    y = "n instances")
```

Finally, merge in population data. This isn't working right now because of naming issues!

````{r merge-population}
pop <- read_csv('https://raw.githubusercontent.com/uclalawcovid19behindbars/Population/main/initial/Merg_Pop.csv') %>%
  mutate(Name = behindbarstools::clean_fac_col_txt(Name, to_upper = TRUE)) %>%
  filter(State == "California")
head(pop)

dat_with_pop <- final_dat %>%
  left_join(pop, by = "Name")

table(is.na(dat_with_pop$Population))
```


```{r write csv}
out <- final_dat %>%
  behindbarstools::reorder_cols(add_missing_cols = TRUE, rm_extra_cols = TRUE) # dont expect anything to change here

## check nothing strange is happening!
skim(out)

write_csv(out, file.path(base_path, "data", "CA-historical-data.csv"))
```
