---
title: "Clean California - COVID-19 Behind Bars Historical Data Cleaning"
author: "Hope Johnson"
date: "4/14/21"
output: html_document
---

```{r package setup, include=FALSE}
##Define package list
Packages<-c("tidyverse", "glue", "assertthat", "stringr", "lubridate",
            "devtools", "magrittr", "skimr")
.packages = Packages
##Install CRAN packages (if not already installed)
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) install.packages(.packages[!.inst])
##Load packages into session 
lapply(.packages, require, character.only=TRUE)
devtools::install_github("uclalawcovid19behindbars/behindbarstools")
help(package=behindbarstools)
```

## Intro & Credits

This script is used to clean one state in the historical data concerning COVID-19 in state and federal prisons. Contributors to the historical data cleaning efforts include Hope Johnson, Michael Everett, Neal Marquez, Chase Hommeyer, Grace DiLaura, and Kalind Parish. Contributors to larger project include Sharon Dolovich, Aaron Littman, Danielle Flores, Poornima Rajeshwar, Victoria Rossi, and many others. 


## Load inputs

Input files: 

* Utilities script
* Historical data

```{r load inputs, echo=FALSE}
base_path <- file.path("~", "UCLA", "code", "historical-data")
data_path <- file.path(base_path, "data", "inputs")
##Load utilities function
util_path <- file.path(base_path, "R", "0-utilities.R")
source(util_path, local = knitr::knit_global())
```


```{r load data, message=FALSE}
df <- load_data(data_path, 
                "11420",
                filter_state = "California") 
df_typed <- type_convert(df)
```

Fill in missing values of `Staff.Deaths` with non-missing values of `Staff.Death`, when those exist. Do the same with `Resident.Death`/`Resident.Deaths`. 

```{r initial cleaning}
df_out <- df_typed %>%
  select(!starts_with("...")) %>%
  select(!starts_with("lots")) %>%
  select(!starts_with("Bad")) %>%
  select(!c("V2", "V4", "V5", "V7", "V8", "V10")) %>%
  select(!c("Facility.", "Coder", "Housing.Type")) %>%
  select_if(~sum(!is.na(.)) > 0) # rm 100% missing cols 
names(df_out) 

df_out %>%
  select(contains("Staff")) %>% 
  names()

df_out %>%
  select(contains("Resident")) %>% 
  names()

df_out %<>% 
  mutate(Resident.Deaths = behindbarstools::coalesce_with_warnings(Resident.Deaths, Resident.Death),
         Staff.Quarantine = behindbarstools::coalesce_with_warnings(Staff.Quarantine, Staff.Quarantined)) 
```

```{r create date var}
df_out <- df_out %>%
  mutate(Date = as_date(sheet_name, format = "%Om.%d.%y"))
```

```{r standardize facility names}
df_cln <- df_out %>%
  mutate(
    Name = str_replace(Name, "-CDCR", "CDCR"),
    Name = str_replace(Name, "CDCRCCHCS", "CDCR CCHCS"),
    Name = str_replace(Name, "CDCR/CCHCS", "CDCR CCHCS"))
```

Standardize facility names and add facility IDs from alternate name spelling crosswalk. 

```{r}
df_mid <- behindbarstools::clean_facility_name(df_cln, debug = TRUE) 
```

```{r filter out federal facilities}
df_nonfederal <- df_mid %>%
  filter(federal_bool == FALSE)
```

```{r check name mismatches}
# show instances where merge didn't identify a clean name
nonmatches <- df_nonfederal %>%
  filter(name_match == FALSE) %>% 
  select(Name) %>%
  unique()

# View(nonmatches)
```

Remove observations where we didn't pick up a clean name.

```{r drop null name observations}
df_filt <- df_nonfederal %>%
  filter(!is.na(scrape_name_clean)) %>%
  filter(name_match == TRUE)
```

Figure out duplicate date/facilities, concatenate those instances from multiple rows into one. This most often occurs because we scraped death data and infections data from separate tables.

```{r concat duplicate date/facilities}
nrow(distinct(df_filt, Date, Name))
see_if(nrow(df_filt) == nrow(distinct(df_filt, Name, Date)))

# if there are two values, sum them 
df_comb <- df_filt %>% 
  behindbarstools::group_by_coalesce(., Name, Facility.ID, Date, 
                                     .ignore = "scrape_name_clean",
                                     .method = "sum") 

assert_that(nrow(df_comb) == nrow(distinct(df_comb, Name, Date)))
```

Filter down and re-order columns in order to row bind them to latest data.

```{r}
df_hist <- behindbarstools::reorder_cols(df_comb)
df_hist_final <- df_hist %>%
  mutate(source = Website,
         Residents.Deaths = Resident.Deaths) %>%
  select(-c(Website, Resident.Deaths, Resident.Death, Count.ID, Facility)) 

# this is ready for merging with post-nov scraped data
df_hist_final <- behindbarstools::reorder_cols(df_hist_final, rm_extra_cols = TRUE) %>%
  select_if(~sum(!is.na(.)) > 0) 

# paring this down for read_scrape_data format
df_hist_towrite <- prep_server_data(df_hist_final, "CA")
skim(df_hist_towrite)
```

Get rid of Residents.Tested variable because it only exists for 2 facilities and might not actually be reflecting the number of tests.
```{r}
df_hist_final <- df_hist_towrite %>%
  select(-Residents.Tested) %>%
  mutate(Staff.Confirmed = ifelse(Date > as.Date("2020-09-03") & Date < as.Date("2020-09-08"),
                                  NA,
                                  Staff.Confirmed),
         Staff.Recovered = ifelse(Date > as.Date("2020-09-03") & Date < as.Date("2020-09-08"),
                                  NA,
                                  Staff.Recovered),
  )
  
```

Write pre-November data to the server!

```{r}
srvr_outfile_name <- '1-pre-november-CA.csv'
write_csv(df_hist_final, file.path(base_path, "data", "pre-nov", srvr_outfile_name))
sync_remote_files(srvr_outfile_name)
```
