---
title: "Clean STATENAME - COVID-19 Behind Bars Historical Data Cleaning"
author: "Hope Johnson"
date: "2/10/21"
output: html_document
---

```{r package setup, include=FALSE}
##Define package list
Packages<-c("tidyverse", "glue", "assertthat", "stringr", "lubridate",
            "devtools", "magrittr", "skimr", "plotly")
.packages = Packages
##Install CRAN packages (if not already installed)
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) install.packages(.packages[!.inst])
##Load packages into session 
lapply(.packages, require, character.only=TRUE)
devtools::install_github("uclalawcovid19behindbars/behindbarstools")
help(package=behindbarstools)

##Define state
state_toclean <- "Alabama"
state_abbrev <- behindbarstools::translate_state(state_toclean, reverse = TRUE)
```

## Intro & Credits

This script is used to clean one state in the historical data concerning COVID-19 in state and federal prisons. Contributors to the historical data cleaning efforts include Hope Johnson, Michael Everett, Neal Marquez, Erika Tyagi, Chase Hommeyer, Grace DiLaura,  and Kalind Parish. Contributors to larger project include Sharon Dolovich, Aaron Littman, Danielle Flores, Poornima Rajeshwar, Victoria Rossi, and many others. 

## Load inputs

Input files: 

* Utilities script
* Historical data

```{r load inputs, echo=FALSE}
base_path <- file.path("~", "UCLA", "code", "historical-data")
data_path <- file.path(base_path, "data", "inputs")
##Load utilities function
util_path <- file.path(base_path, "R", "0-utilities.R")
source(util_path, local = knitr::knit_global())
```

Load in all the historical data for this state!

```{r load data, message=FALSE}
df <- load_data(data_path, 
                "11420",
                filter_state = state_toclean) 
df_typed <- type_convert(df)
```

Fill in missing values of `Staff.Deaths` with non-missing values of `Staff.Death`, when those exist. Do the same with `Resident.Death`/`Resident.Deaths`. 

```{r initial cleaning}
df_out <- df_typed %>%
  select(!starts_with("...")) %>%
  select(!starts_with("lots")) %>%
  select(!starts_with("Bad")) %>%
  select(!c("V2", "V4", "V5", "V7", "V8", "V10")) %>%
  select(!c("Facility.", "Coder", "Housing.Type")) %>%
  select_if(~sum(!is.na(.)) > 0) %>% # rm 100% missing cols 
  mutate(Zipcode = as.character(Zipcode),
         County.FIPS = as.character(County.FIPS))

# if there are any other similarly-named columns in the results below, pay attention!
df_out %>%
  select(contains("Staff")) %>% 
  names()

df_out %>%
  select(contains("Resident")) %>% 
  names()
```

```{r}
df_out %<>% 
  mutate(Staff.Deaths = behindbarstools::coalesce_with_warnings(Staff.Deaths, Staff.Death),
         Resident.Deaths = behindbarstools::coalesce_with_warnings(Resident.Deaths, Resident.Death)) 
```

```{r create date var}
df_out <- df_out %>%
  mutate(Date = as_date(sheet_name, format = "%Om.%d.%y"))
```

```{r standardize facility names}
df_mid <- behindbarstools::clean_facility_name(df_out, debug = TRUE) 
```

```{r filter out federal facilities}
df_mid <- df_mid %>%
  filter(federal_bool == FALSE)
```

```{r facility merge checks, include = FALSE}
# show instances where merge didn't identify a clean name
# check if it's in fac_data
df_mid %>%
  filter(name_match == FALSE) %>% 
  select(scrape_name_clean) %>%
  unique()
```

Figure out duplicate date/facilities, concatenate those instances from multiple rows into one. This most often occurs because we scraped death data and infections data from separate tables.

```{r concat duplicate date/facilities}
nrow(distinct(df_mid, Date, Name))
see_if(nrow(df_mid) == nrow(distinct(df_mid, Name, Date)))

# if there are two values, sum them 
df_comb <- df_mid %>% 
  behindbarstools::group_by_coalesce(., Name, Facility.ID, Date, 
                                     .ignore = "scrape_name_clean",
                                     .method = "sum") 

assert_that(nrow(df_comb) == nrow(distinct(df_comb, Name, Date)))
```

Filter down and re-order columns in order to row bind them to latest data.

```{r}
df_hist <- behindbarstools::reorder_cols(df_comb)
df_hist_final <- df_hist %>%
  mutate(source = Website,
         Residents.Deaths = Resident.Deaths) %>%
  select(-c(Website, Resident.Deaths, Resident.Death, Count.ID, Facility)) 

df_hist_final <- behindbarstools::reorder_cols(df_hist_final, rm_extra_cols = TRUE) %>%
  select_if(~sum(!is.na(.)) > 0) # rm all NA

```

Add in more recently scraped data (from November until present). 

First, read it in and clean it. 

```{r add in recently scraped data}
recent_dat <- behindbarstools::read_scrape_data(all_dates = TRUE, 
                                                state = state_toclean, 
                                                debug = TRUE) %>%
  filter(Jurisdiction != "federal",
       Date > as.Date('2020-11-04'))
```

Look at facilities that aren't in facility name crosswalk and/or facility data sheet.

```{r}
## view facility names without a match in the xwalk
recent_dat %>%
  filter(name_match == "FALSE") %>% 
  select(scrape_name_clean) %>%
  unique()

recent_final <- behindbarstools::reorder_cols(recent_dat, rm_extra_cols = TRUE)
```

Then, bind it to the historical cleaned data from this script. 

```{r bind historical and recent}
all_equal(df_hist_final, recent_final, ignore_col_order = FALSE)
all_dat <- bind_rows(df_hist_final, recent_final)
n_distinct(all_dat$Name)

# make sure there's only 1 clean name per facility ID! 
all_dat %>% 
  group_by(Facility.ID) %>% 
  summarise(n_name = n_distinct(Name)) %>% 
  filter(n_name > 1)

# make sure there's only 1 facility ID per clean name!
all_dat %>% 
    group_by(Name) %>% 
    summarise(n_fac_ID = n_distinct(Facility.ID)) %>% 
    filter(n_fac_ID > 1)
```
Merge in facility information (address, city, county, etc.).

```{r}
fac_info <- behindbarstools::read_fac_info() 
final_dat <- left_join(all_dat, fac_info, 
                  by = "Facility.ID",
                  suffix = c(".x", "")) 

# coalesce important columns, prioritizing facility_data info for rows that have both
final_dat %<>% 
  mutate(State = behindbarstools::coalesce_with_warnings(State, State.x),
         Address = behindbarstools::coalesce_with_warnings(Address, Address.x),
         Name = behindbarstools::coalesce_with_warnings(Name, Name.x),
         Zipcode = behindbarstools::coalesce_with_warnings(Zipcode, Zipcode.x),
         City = behindbarstools::coalesce_with_warnings(City, City.x),
         County = behindbarstools::coalesce_with_warnings(County, County.x)) %>%
  select(-ends_with(".x"))
```

Make plots for the data, and take a look at any weirdness. 

```{r plot cases/deaths}
# create a testing file 
testing_file <- flag_noncumulative_cases(final_dat, Name)
testing_file <- flag_noncumulative_deaths(testing_file, Name, Residents.Deaths)

# lag cases overall 
all_cases <- testing_file %>%
  filter(Name != "STATEWIDE") %>%
  ggplot(data = ., 
         aes(x = Date, y = lag_change_cases, group = Name)) +
    geom_line(alpha=0.6 , size=.5, color = "black") +
         scale_x_date(date_minor_breaks = "1 month", date_labels = "%m/%y", 
                      date_breaks = "1 month") + 
    labs(x = "Date",
      y = "lag_change_cases")

ggplotly(all_cases)

# lag deaths overall
all_deaths <- testing_file %>%
  filter(Name != "STATEWIDE") %>%
  ggplot(data = ., 
         aes(x = Date, y = lag_change_deaths, group = Name)) +
    geom_line(alpha=0.6 , size=.5, color = "black") +
    scale_x_date(date_labels = "%m") + 
    labs(x = "Date",
      y = "lag_change_deaths")
ggplotly(all_deaths)

```

Make facility-specific plots, and take a look at any weirdness.

```{r}
# will throw a warning if path already exists
plot_path <- file.path(base_path, "plots", state_abbrev)
dir.create(file.path(plot_path))
dir.create(file.path(plot_path, "cases"))
dir.create(file.path(plot_path, "deaths"))

# plot lag counts by facility
lag_case_plots <- plot_lags(testing_file, "Date", 
                            y_var = "lag_change_cases",
                            grp_var = Name)
lag_case_plots$plot

# save lag case plots
# for (i in 1:nrow(lag_case_plots)){
#   facility_name <- lag_case_plots$Name[[i]]
#   ggsave(paste0(facility_name, "_LagChangeCases.png"), lag_case_plots$plot[[i]],
#          path = file.path(base_path, "plots", state_abbrev, "cases"))
# }

lag_death_plots <- plot_lags(testing_file, "Date", 
                            y_var = "lag_change_deaths",
                            grp_var = Name)
# save lag death plots
# for (i in 1:nrow(lag_death_plots)){
#   facility_name <- lag_death_plots$Name[[i]]
#   ggsave(paste0(facility_name, "_LagChangeDeaths.png"), lag_death_plots$plot[[i]],
#          path = file.path(base_path, "plots", state_abbrev, "deaths"))
# }
```

Find date spans / week spans with no data. In instances where the count went down by one, it could be that a PDF was misread. 

```{r}
dates <- final_dat %>%
  arrange(Date) %>%
  count(Date)
dates

ggplot(data = dates, 
       aes(x = Date, y = n)) +
  geom_bar(stat="identity") +
  labs(x = "Date",
    y = "n instances")
```

```{r write csv}
out <- final_dat %>%
  reorder_historical_cols(rm_extra_cols = TRUE)

## check nothing strange is happening!
skim(out)

outfile_name <- glue('{state_abbrev}-historical-data.csv')
write_csv(out, file.path(base_path, "data", outfile_name))
```
